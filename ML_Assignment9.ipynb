{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Assignment9.ipynb",
      "provenance": [],
      "mount_file_id": "1QAlW4-0jJSYIwVYUsouEEyp5P_ZCba0o",
      "authorship_tag": "ABX9TyN6jdKtBm+9/Errloh0itC4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/97-63/Sabudh_Foundation_projects/blob/master/ML_Assignment9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN_9MBFXHjq-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWw9-1M4Hug3",
        "colab_type": "text"
      },
      "source": [
        "**PROBLEM 1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SEdgCpLHyBY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(filepath):\n",
        "  df = pd.read_csv(filepath)\n",
        "  df.head()\n",
        "  Y = df['5'].to_numpy()\n",
        "  del df['5']\n",
        "  X=df.to_numpy()\n",
        "  return X, Y\n",
        "\n",
        "X, y = load_data(\"/content/drive/My Drive/SabhudhFoundation/mnist_train.csv\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\\\n",
        "                X, y, test_size=0.3, random_state=42)\n",
        "#One hot encoding of training labels \n",
        "Labels=pd.get_dummies(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RBaOoZFI0gb",
        "colab_type": "text"
      },
      "source": [
        "**PROBLEM 2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NJzkYLJI2qH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Perceptron():\n",
        "    def __init__(self,x,y):\n",
        "        \"\"\"\n",
        "        x is 2d array of input images\n",
        "        y are one hot encoded labels \n",
        "        \"\"\"\n",
        "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
        "        self.y=y\n",
        "        self.weights=[]\n",
        "        self.bias=[]\n",
        "        self.outputs=[]\n",
        "        self.derivatives=[]\n",
        "        self.activations=[]\n",
        "        \n",
        "    def connect(self,layer1,layer2):\n",
        "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
        "        #Initialise weights,derivatives and activation lists\n",
        "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "\n",
        "\n",
        "    def softmax(self,z):\n",
        "        e=np.exp(z)\n",
        "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
        "    \n",
        "    def max_log_likelihood(self,y_pred,y):\n",
        "        \"\"\"cross entropy\"\"\"\n",
        "        return y*np.log(y_pred)\n",
        "    \n",
        "    def delta_mll(self,y,y_pred):\n",
        "        \"\"\"derivative of cross entropy\"\"\"\n",
        "        #return y*(y_pred-1)\n",
        "        return y_pred-y\n",
        "    \n",
        "    def forward_pass(self,x,y,weights,bias):\n",
        "        cost=0\n",
        "        self.outputs=[]\n",
        "        for i in range(len(weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            self.outputs.append(x) #append without adding ones array\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
        "            x=self.softmax(z)\n",
        "        self.outputs.append(x)\n",
        "        self.y_pred=x\n",
        "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
        "        cost=np.mean(np.sum(temp,axis=1))\n",
        "        return cost\n",
        "    \n",
        "    \n",
        "    def backward_pass(self,y,lr):\n",
        "        for i in range(len(self.weights)-1,-1,-1):\n",
        "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
        "            prev_term=self.delta_mll(y,self.y_pred)  \n",
        "            # derivatives follow specific order,last three terms added new,rest from previous term  \n",
        "            self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
        "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "    \n",
        "    def train(self,batches,lr=1e-3,epoch=10):\n",
        "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
        "        for epochs in range(epoch):\n",
        "            samples=len(self.x)\n",
        "            c=0\n",
        "            for i in range(batches):\n",
        "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
        "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
        "              \n",
        "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
        "              self.backward_pass(y_batch,lr)\n",
        "            print(epochs,c/batches)\n",
        "    \n",
        "    def predict(self,x):\n",
        "        \"\"\"input: x_test values\"\"\"\n",
        "        x=x/255\n",
        "        for i in range(len(self.weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
        "            x=self.softmax(z) \n",
        "        return np.argmax(x,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG2J_PotI_bK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "outputId": "61ff9dcc-70a9-486a-a01c-a75f6b2065ee"
      },
      "source": [
        "n=Perceptron(X_train,Labels)\n",
        "n.connect(X_train,Labels)\n",
        "n.train(batches=1000,lr=0.26,epoch=30)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.002239349928913165\n",
            "1 0.0027296631970205386\n",
            "2 0.001990446405959487\n",
            "3 0.002100631182741135\n",
            "4 0.0023232995336540233\n",
            "5 0.0019062521796497883\n",
            "6 0.0018963904595530664\n",
            "7 0.0017805465105291695\n",
            "8 0.001539316975491055\n",
            "9 0.0015152943215683157\n",
            "10 0.0015673969891914059\n",
            "11 0.0015013590771661104\n",
            "12 0.0008447643648544078\n",
            "13 0.0005709956816006085\n",
            "14 0.0005635303014642812\n",
            "15 0.0008629935204407092\n",
            "16 0.00023066756205606458\n",
            "17 0.0008385412250532064\n",
            "18 0.00026996510759111104\n",
            "19 0.00027849450500859945\n",
            "20 0.00020260197531065448\n",
            "21 0.00016405684424922515\n",
            "22 2.5131903742029834e-05\n",
            "23 8.546410561907918e-05\n",
            "24 9.105632548128041e-05\n",
            "25 0.00010607489933862235\n",
            "26 0.0002562879005430349\n",
            "27 2.2036765640118167e-05\n",
            "28 3.7392654455351624e-05\n",
            "29 1.5888157849050834e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0WPYsuyJCuf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "539c3187-198d-409f-c099-40bebd19a81e"
      },
      "source": [
        "pred=n.predict(X_test)\n",
        "np.bincount(n.predict(X_test)),np.bincount(y_test)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([310, 334, 293, 342, 336, 292, 283, 314, 223, 273]),\n",
              " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfe3cNj4Jj8Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a0bfca0c-93b6-4dd3-a4b8-fa16bd5b0721"
      },
      "source": [
        "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy is 88.1 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2eKn8WYKAXW",
        "colab_type": "text"
      },
      "source": [
        "**PROBLEM 3**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOC6cFHNKCbU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "ee5732f0-144e-400f-c5ad-4050a227daba"
      },
      "source": [
        "class Layer():\n",
        "    \"\"\"\n",
        "    size: Number of nodes in the hidden layer \n",
        "    activation: name of activation function for the layer\n",
        "    \"\"\"\n",
        "    def __init__(self,size,activation='sigmoid'): \n",
        "        self.shape=(1,size)\n",
        "        self.activation=activation\n",
        "                \n",
        "class SingleLayerNeuralNetwork():\n",
        "    def __init__(self,x,y):\n",
        "        \"\"\"\n",
        "        x is 2d array of input images\n",
        "        y are one hot encoded labels \n",
        "        \"\"\"\n",
        "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
        "        self.y=y\n",
        "        self.weights=[]\n",
        "        self.bias=[]\n",
        "        self.outputs=[]\n",
        "        self.derivatives=[]\n",
        "        self.activations=[]\n",
        "        \n",
        "    def connect(self,layer1,layer2):\n",
        "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
        "        #Initialise weights,derivatives and activation lists\n",
        "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        if isinstance(layer2,Layer):\n",
        "            self.activations.append(layer2.activation)\n",
        "            \n",
        "    def activation(self,name,z,derivative=False):\n",
        "        \n",
        "        #implementation of various activation functions and their derivatives\n",
        "        if name=='sigmoid':\n",
        "            if derivative==False:\n",
        "                return 1/(1+np.exp(-z))\n",
        "            else:\n",
        "                return z*(1-z)\n",
        "        \n",
        "    def softmax(self,z):\n",
        "        e=np.exp(z)\n",
        "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
        "    \n",
        "    def max_log_likelihood(self,y_pred,y):\n",
        "        \"\"\"cross entropy\"\"\"\n",
        "        return y*np.log(y_pred)\n",
        "    \n",
        "    def delta_mll(self,y,y_pred):\n",
        "        \"\"\"derivative of cross entropy\"\"\"\n",
        "        #return y*(y_pred-1)\n",
        "        return y_pred-y\n",
        "    \n",
        "    def forward_pass(self,x,y,weights,bias):\n",
        "        cost=0\n",
        "        self.outputs=[]\n",
        "        for i in range(len(weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            self.outputs.append(x) #append without adding ones array\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
        "            if i==len(weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        self.outputs.append(x)\n",
        "        self.y_pred=x\n",
        "        \n",
        "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
        "        cost=np.mean(np.sum(temp,axis=1))\n",
        "        return cost\n",
        "    \n",
        "    \n",
        "    def backward_pass(self,y,lr):\n",
        "        for i in range(len(self.weights)-1,-1,-1):\n",
        "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
        "            if i==len(self.weights)-1:\n",
        "                prev_term=self.delta_mll(y,self.y_pred)  \n",
        "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
        "            else:\n",
        "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
        "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "                \n",
        "    \n",
        "    def train(self,batches,lr=1e-3,epoch=10):\n",
        "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
        "        for epochs in range(epoch):\n",
        "            samples=len(self.x)\n",
        "            c=0\n",
        "            for i in range(batches):\n",
        "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
        "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
        "              \n",
        "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
        "              self.backward_pass(y_batch,lr)\n",
        "            print(epochs,c/batches)\n",
        "    \n",
        "    def predict(self,x):\n",
        "        \"\"\"input: x_test values\"\"\"\n",
        "        x=x/255\n",
        "        for i in range(len(self.weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
        "            if i==len(self.weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "\n",
        "        return np.argmax(x,axis=1)\n",
        "n=SingleLayerNeuralNetwork(X_train,Labels)\n",
        "l1=Layer(100)\n",
        "n.connect(X_train,l1)\n",
        "n.connect(l1,Labels)\n",
        "n.train(batches=1000,lr=0.1,epoch=50)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.0007902057614101923\n",
            "1 0.0006399978799779555\n",
            "2 0.000624359057765749\n",
            "3 0.0006102601834848754\n",
            "4 0.0005570102129615644\n",
            "5 0.0004917474627937354\n",
            "6 0.0004244343567604269\n",
            "7 0.0003621727945381529\n",
            "8 0.0003080259788858708\n",
            "9 0.00025737226743357476\n",
            "10 0.00021014520339760108\n",
            "11 0.00016886220322194314\n",
            "12 0.00013549239694172655\n",
            "13 0.00010924291440966997\n",
            "14 8.91101649441811e-05\n",
            "15 7.425207584025295e-05\n",
            "16 6.294429305097286e-05\n",
            "17 5.401235752140248e-05\n",
            "18 4.68523330375756e-05\n",
            "19 4.110395686268736e-05\n",
            "20 3.650763402311559e-05\n",
            "21 3.2923089965810744e-05\n",
            "22 3.0190471253248254e-05\n",
            "23 2.797137838514516e-05\n",
            "24 2.608026047767076e-05\n",
            "25 2.4485011343821465e-05\n",
            "26 2.3144899585283473e-05\n",
            "27 2.197772156696064e-05\n",
            "28 2.0908886971781623e-05\n",
            "29 1.991104387709434e-05\n",
            "30 1.898095862870835e-05\n",
            "31 1.81256770607739e-05\n",
            "32 1.7353185501182582e-05\n",
            "33 1.6663343056804633e-05\n",
            "34 1.6049057072539645e-05\n",
            "35 1.5500509546247183e-05\n",
            "36 1.5007795566837444e-05\n",
            "37 1.4562008049270354e-05\n",
            "38 1.4155528154486523e-05\n",
            "39 1.3781987740523618e-05\n",
            "40 1.3436120098846346e-05\n",
            "41 1.3113589046454276e-05\n",
            "42 1.2810830300598677e-05\n",
            "43 1.2524915344289405e-05\n",
            "44 1.2253438498408156e-05\n",
            "45 1.1994424535132982e-05\n",
            "46 1.1746253342897068e-05\n",
            "47 1.1507598333193164e-05\n",
            "48 1.1277375791890394e-05\n",
            "49 1.1054702939653527e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7Np6Mo2KMH_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "78f84f1d-0ae7-4943-baf9-99a455b477f2"
      },
      "source": [
        "pred=n.predict(X_test)\n",
        "np.bincount(n.predict(X_test)),np.bincount(y_test)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([306, 334, 294, 327, 310, 262, 287, 328, 257, 295]),\n",
              " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsyBMNTfKVvF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ffcd9b19-46c5-4e9e-fa5c-05fe3360d945"
      },
      "source": [
        "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy is 91.8 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NN3oZhYKbkW",
        "colab_type": "text"
      },
      "source": [
        "**PROBLEM 4**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmvqFVkpKeNW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "845c38d7-d07a-4515-d020-7cb97ca20de6"
      },
      "source": [
        "class Layer():\n",
        "    \"\"\"\n",
        "    size: Number of nodes in the hidden layer \n",
        "    activation: name of activation function for the layer\n",
        "    \"\"\"\n",
        "    def __init__(self,size,activation='sigmoid'): \n",
        "        self.shape=(1,size)\n",
        "        self.activation=activation\n",
        "                \n",
        "class DoubleLayerNeuralNetwork():\n",
        "    def __init__(self,x,y):\n",
        "        \"\"\"\n",
        "        x is 2d array of input images\n",
        "        y are one hot encoded labels \n",
        "        \"\"\"\n",
        "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
        "        self.y=y\n",
        "        self.weights=[]\n",
        "        self.bias=[]\n",
        "        self.outputs=[]\n",
        "        self.derivatives=[]\n",
        "        self.activations=[]\n",
        "        \n",
        "    def connect(self,layer1,layer2):\n",
        "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
        "        #Initialise weights,derivatives and activation lists\n",
        "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        if isinstance(layer2,Layer):\n",
        "            self.activations.append(layer2.activation)\n",
        "            \n",
        "    def activation(self,name,z,derivative=False):\n",
        "        \n",
        "        #implementation of various activation functions and their derivatives\n",
        "        if name=='sigmoid':\n",
        "            if derivative==False:\n",
        "                return 1/(1+np.exp(-z))\n",
        "            else:\n",
        "                return z*(1-z)\n",
        "        \n",
        "    def softmax(self,z):\n",
        "        e=np.exp(z)\n",
        "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
        "    \n",
        "    def max_log_likelihood(self,y_pred,y):\n",
        "        \"\"\"cross entropy\"\"\"\n",
        "        return y*np.log(y_pred)\n",
        "    \n",
        "    def delta_mll(self,y,y_pred):\n",
        "        \"\"\"derivative of cross entropy\"\"\"\n",
        "        #return y*(y_pred-1)\n",
        "        return y_pred-y\n",
        "    \n",
        "    def forward_pass(self,x,y,weights,bias):\n",
        "        cost=0\n",
        "        self.outputs=[]\n",
        "        for i in range(len(weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            self.outputs.append(x) #append without adding ones array\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
        "            if i==len(weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        self.outputs.append(x)\n",
        "        self.y_pred=x\n",
        "        \n",
        "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
        "        cost=np.mean(np.sum(temp,axis=1))\n",
        "        return cost\n",
        "    \n",
        "    \n",
        "    def backward_pass(self,y,lr):\n",
        "        for i in range(len(self.weights)-1,-1,-1):\n",
        "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
        "            if i==len(self.weights)-1:\n",
        "                prev_term=self.delta_mll(y,self.y_pred)  \n",
        "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
        "            else:\n",
        "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
        "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "                \n",
        "    \n",
        "    def train(self,batches,lr=1e-3,epoch=10):\n",
        "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
        "        for epochs in range(epoch):\n",
        "            samples=len(self.x)\n",
        "            c=0\n",
        "            for i in range(batches):\n",
        "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
        "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
        "              \n",
        "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
        "              self.backward_pass(y_batch,lr)\n",
        "            print(epochs,c/batches)\n",
        "    \n",
        "    def predict(self,x):\n",
        "        \"\"\"input: x_test values\"\"\"\n",
        "        x=x/255\n",
        "        for i in range(len(self.weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
        "            if i==len(self.weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        return np.argmax(x,axis=1)\n",
        "n=DoubleLayerNeuralNetwork(X_train,Labels)\n",
        "l1=Layer(100)\n",
        "l2=Layer(100)\n",
        "n.connect(X_train,l1)\n",
        "n.connect(l1,l2)\n",
        "n.connect(l2,Labels)\n",
        "n.train(batches=1000,lr=0.1,epoch=20)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.0018620357671648082\n",
            "1 0.0011322428787106198\n",
            "2 0.0009916757424432506\n",
            "3 0.0007428194654048642\n",
            "4 0.00048515718605790775\n",
            "5 0.0002066756645931535\n",
            "6 6.166591693861435e-05\n",
            "7 3.628547983093653e-05\n",
            "8 2.7653230519084396e-05\n",
            "9 2.1062132582390402e-05\n",
            "10 1.7309788547021367e-05\n",
            "11 1.5667121930526403e-05\n",
            "12 1.4986486500401386e-05\n",
            "13 1.4816957975401764e-05\n",
            "14 1.511068528304183e-05\n",
            "15 1.5488545391103988e-05\n",
            "16 1.546234316638769e-05\n",
            "17 1.4855707811471385e-05\n",
            "18 1.3872606788960509e-05\n",
            "19 1.2800394470791782e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3VGra0wKrji",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ebfe6b96-b49d-4be3-8ca5-06cc8c5c102a"
      },
      "source": [
        "pred=n.predict(X_test)\n",
        "np.bincount(n.predict(X_test)),np.bincount(y_test)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([303, 326, 291, 343, 317, 254, 308, 323, 251, 284]),\n",
              " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_zBrYQHKtgg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0d514f5c-e6c3-43df-b486-16daf9aaa5de"
      },
      "source": [
        "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy is 90.7 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFCidpEMKuzC",
        "colab_type": "text"
      },
      "source": [
        "**PROBLEM 5**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lewN4UP3KzIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer():\n",
        "    \"\"\"\n",
        "    size: Number of nodes in the hidden layer \n",
        "    activation: name of activation function for the layer\n",
        "    \"\"\"\n",
        "    def __init__(self,size,activation='sigmoid'): \n",
        "        self.shape=(1,size)\n",
        "        self.activation=activation\n",
        "                \n",
        "class NeuralNetworkActivations():\n",
        "    def __init__(self,x,y):\n",
        "        \"\"\"\n",
        "        x is 2d array of input images\n",
        "        y are one hot encoded labels \n",
        "        \"\"\"\n",
        "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
        "        self.y=y\n",
        "        self.weights=[]\n",
        "        self.bias=[]\n",
        "        self.outputs=[]\n",
        "        self.derivatives=[]\n",
        "        self.activations=[]\n",
        "        \n",
        "    def connect(self,layer1,layer2):\n",
        "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
        "        #Initialise weights,derivatives and activation lists\n",
        "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        if isinstance(layer2,Layer):\n",
        "            self.activations.append(layer2.activation)\n",
        "            \n",
        "    def activation(self,name,z,derivative=False):\n",
        "        \n",
        "        #implementation of various activation functions and their derivatives\n",
        "        if name=='sigmoid':\n",
        "            if derivative==False:\n",
        "                return 1/(1+np.exp(-z))\n",
        "            else:\n",
        "                return z*(1-z)\n",
        "        elif name=='relu':\n",
        "            if derivative==False:\n",
        "                return np.maximum(0.0,z)\n",
        "            else:\n",
        "              z[z<=0] = 0.0\n",
        "              z[z>0] = 1.0\n",
        "              return z\n",
        "        elif name=='tanh':\n",
        "          if derivative==False:\n",
        "                return np.tanh(z)\n",
        "          else:\n",
        "                return 1.0 - (np.tanh(z)) ** 2\n",
        "        \n",
        "    def softmax(self,z):\n",
        "        e=np.exp(z)\n",
        "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
        "    \n",
        "    def max_log_likelihood(self,y_pred,y):\n",
        "        \"\"\"cross entropy\"\"\"\n",
        "        return y*np.log(y_pred)\n",
        "    \n",
        "    def delta_mll(self,y,y_pred):\n",
        "        \"\"\"derivative of cross entropy\"\"\"\n",
        "        #return y*(y_pred-1)\n",
        "        return y_pred-y\n",
        "    \n",
        "    def forward_pass(self,x,y,weights,bias):\n",
        "        cost=0\n",
        "        self.outputs=[]\n",
        "        for i in range(len(weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            self.outputs.append(x) #append without adding ones array\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
        "            if i==len(weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        self.outputs.append(x)\n",
        "        self.y_pred=x\n",
        "        \n",
        "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
        "        cost=np.mean(np.sum(temp,axis=1))\n",
        "        return cost\n",
        "    \n",
        "    \n",
        "    def backward_pass(self,y,lr):\n",
        "        for i in range(len(self.weights)-1,-1,-1):\n",
        "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
        "            if i==len(self.weights)-1:\n",
        "                prev_term=self.delta_mll(y,self.y_pred)  \n",
        "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
        "            else:\n",
        "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
        "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "    \n",
        "    def train(self,batches,lr=1e-3,epoch=10):\n",
        "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
        "        for epochs in range(epoch):\n",
        "            samples=len(self.x)\n",
        "            c=0\n",
        "            for i in range(batches):\n",
        "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
        "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
        "              \n",
        "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
        "              self.backward_pass(y_batch,lr)\n",
        "            print(epochs,c/batches)\n",
        "    \n",
        "    def predict(self,x):\n",
        "        \"\"\"input: x_test values\"\"\"\n",
        "        x=x/255\n",
        "        for i in range(len(self.weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
        "            if i==len(self.weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        return np.argmax(x,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QCXrPoYK5Az",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "f446838b-29b5-47b3-86a2-5e3eff465258"
      },
      "source": [
        "n=NeuralNetworkActivations(X_train,Labels)\n",
        "l1=Layer(100,'sigmoid')\n",
        "l2=Layer(50, 'tanh')\n",
        "n.connect(X_train,l1)\n",
        "n.connect(l1,l2)\n",
        "n.connect(l2,Labels)\n",
        "n.train(batches=1000,lr=0.1,epoch=20)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.0003814039981850383\n",
            "1 0.00037466461341413744\n",
            "2 0.0004722411260161435\n",
            "3 0.00022939842637322656\n",
            "4 0.0005847940598021915\n",
            "5 0.000456080985857718\n",
            "6 0.0003383881992407072\n",
            "7 7.860988871623613e-05\n",
            "8 0.0001392443280563125\n",
            "9 0.00012014609441729543\n",
            "10 0.0002018234407641289\n",
            "11 3.6424187863476683e-06\n",
            "12 0.00014457188158054698\n",
            "13 2.0637654936927024e-05\n",
            "14 2.053248568047198e-05\n",
            "15 4.730796521377035e-06\n",
            "16 2.9024874015871588e-05\n",
            "17 4.6407691315587124e-05\n",
            "18 4.3757067084894485e-05\n",
            "19 0.0003453697069349692\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJwfdhOaK7ec",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9f7766c5-2f49-4378-f5d8-dec0b30e4983"
      },
      "source": [
        "pred=n.predict(X_test)\n",
        "np.bincount(n.predict(X_test)),np.bincount(y_test)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([307, 337, 312, 310, 315, 280, 284, 329, 248, 278]),\n",
              " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZF16fqoK9Wr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "08c0fa4e-266a-4305-d5ef-ebad3a07b474"
      },
      "source": [
        "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy is 92.56666666666666 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec7QFqj1K-8C",
        "colab_type": "text"
      },
      "source": [
        "**PROBLEM 6**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK5nvRqfLFje",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer():\n",
        "    \"\"\"\n",
        "    size: Number of nodes in the hidden layer \n",
        "    activation: name of activation function for the layer\n",
        "    \"\"\"\n",
        "    def __init__(self,size,activation='sigmoid'): \n",
        "        self.shape=(1,size)\n",
        "        self.activation=activation\n",
        "                \n",
        "class NeuralNetworkMomentum():\n",
        "    def __init__(self,x,y):\n",
        "        \"\"\"\n",
        "        x is 2d array of input images\n",
        "        y are one hot encoded labels \n",
        "        \"\"\"\n",
        "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
        "        self.y=y\n",
        "        self.weights=[]\n",
        "        self.bias=[]\n",
        "        self.outputs=[]\n",
        "        self.derivatives=[]\n",
        "        self.activations=[]\n",
        "        self.delta_weights=[]\n",
        "        self.delta_bias=[]\n",
        "        \n",
        "    def connect(self,layer1,layer2):\n",
        "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
        "        #Initialise weights,derivatives and activation lists\n",
        "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.delta_weights.append(np.zeros((layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.delta_bias.append(np.zeros((layer1.shape[1]+1,layer2.shape[1])))\n",
        "        if isinstance(layer2,Layer):\n",
        "            self.activations.append(layer2.activation)\n",
        "            \n",
        "    def activation(self,name,z,derivative=False):\n",
        "        \n",
        "        #implementation of various activation functions and their derivatives\n",
        "        if name=='sigmoid':\n",
        "            if derivative==False:\n",
        "                return 1/(1+np.exp(-z))\n",
        "            else:\n",
        "                return z*(1-z)\n",
        "        elif name=='relu':\n",
        "            if derivative==False:\n",
        "                return np.maximum(0.0,z)\n",
        "            else:\n",
        "              z[z<=0] = 0.0\n",
        "              z[z>0] = 1.0\n",
        "              return z\n",
        "        elif name=='tanh':\n",
        "          if derivative==False:\n",
        "                return np.tanh(z)\n",
        "          else:\n",
        "                return 1.0 - (np.tanh(z)) ** 2\n",
        "        \n",
        "    def softmax(self,z):\n",
        "        e=np.exp(z)\n",
        "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
        "    \n",
        "    def max_log_likelihood(self,y_pred,y):\n",
        "        \"\"\"cross entropy\"\"\"\n",
        "        return y*np.log(y_pred)\n",
        "    \n",
        "    def delta_mll(self,y,y_pred):\n",
        "        \"\"\"derivative of cross entropy\"\"\"\n",
        "        #return y*(y_pred-1)\n",
        "        return y_pred-y\n",
        "    \n",
        "    def forward_pass(self,x,y,weights,bias):\n",
        "        cost=0\n",
        "        self.outputs=[]\n",
        "        for i in range(len(weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            self.outputs.append(x) #append without adding ones array\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
        "            if i==len(weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        self.outputs.append(x)\n",
        "        self.y_pred=x\n",
        "        \n",
        "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
        "        cost=np.mean(np.sum(temp,axis=1))\n",
        "        return cost\n",
        "    \n",
        "    \n",
        "    def backward_pass(self,y,lr,momentum=False,beta=0.9):\n",
        "        for i in range(len(self.weights)-1,-1,-1):\n",
        "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
        "            if i==len(self.weights)-1:\n",
        "                prev_term=self.delta_mll(y,self.y_pred)  \n",
        "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
        "            else:\n",
        "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
        "            if momentum:\n",
        "                self.delta_weights[i]=beta*self.delta_weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "                self.delta_bias[i]=beta*self.delta_bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "                self.weights[i]=self.weights[i]+self.delta_weights[i]\n",
        "                self.bias[i]=self.bias[i]+self.delta_bias[i]\n",
        "            else:\n",
        "                self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "                self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "    \n",
        "    def train(self,batches,lr=1e-3,epoch=10,beta):\n",
        "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
        "        for epochs in range(epoch):\n",
        "            samples=len(self.x)\n",
        "            c=0\n",
        "            for i in range(batches):\n",
        "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
        "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
        "              \n",
        "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
        "              self.backward_pass(y_batch,lr,momentum=True,beta)\n",
        "            print(epochs,c/batches)\n",
        "    \n",
        "    def predict(self,x):\n",
        "        \"\"\"input: x_test values\"\"\"\n",
        "        x=x/255\n",
        "        for i in range(len(self.weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
        "            if i==len(self.weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        return np.argmax(x,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5ILg4qQLH32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n=NeuralNetworkMomentum(X_train,Labels)\n",
        "l1=Layer(100,'sigmoid')\n",
        "l2=Layer(50, 'tanh')\n",
        "n.connect(X_train,l1)\n",
        "n.connect(l1,l2)\n",
        "n.connect(l2,Labels)\n",
        "n.train(batches=500,lr=0.1,epoch=20,beta=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14LXE02kLJ6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "pred=n.predict(X_test)\n",
        "np.bincount(n.predict(X_test)),np.bincount(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJGAJf20LL31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}